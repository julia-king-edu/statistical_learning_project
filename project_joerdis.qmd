---
title: "Final Project"
author: "JÃ¶rdis Strack, Julia King"
format: html
editor: source
---

## 0. Setup Chunk

Text

```{r}
#| output: false
if (!require("pacman")) install.packages("pacman")
pacman::p_load(readr, tidyverse, class, caret, bench, DataExplorer, randomForest, PRROC)
```

## 1. Data Exploration

Text

```{r}
#| warning: false
# import dataset
data = read_csv("data/BreastCancer.csv", show_col_types = FALSE)
```

```{r}
# first overview
head(data)
```

```{r}
# explore features and labels
str(data)
```

```{r}
# continue exploration
dim(data)
```

```{r}
# count missing values per column
missing_count_per_column <- colSums(is.na(data))
missing_count_per_column

# INTERPRETATION: There are only missing values for variable
# data$...33 -> caused by trailing comma in csv

data <- subset(data, select = -c(...33))
```

```{r}
#| fig-width: 9
#| fig-height: 7
# visualize distribution of classes

# visualize means
data_means = pivot_longer(data %>% select(3:12), 
                                  everything(), 
                                  names_to = "Variable", 
                                  values_to = "Value")
ggplot(data_means, aes(x = Value)) +
  geom_density(fill = "skyblue", color = "black", alpha = 0.5) +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the variable means")
```

Most means seem to be right-skewed. Thus, they will likely require some form of correction. Taking the logarithm is infeasible because of 0 values. Thus, square roots are shown below:

```{r}
#| fig-width: 9
#| fig-height: 7
ggplot(data_means, aes(x = sqrt(Value))) +
  geom_density(fill = "red", color = "black", alpha = 0.5) +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the square root transformation of the variable means")
```

```{r}
#| fig-width: 9
#| fig-height: 7
# visualize standard errors
data_variances = pivot_longer(data %>% select(13:22), 
                                  everything(), 
                                  names_to = "Variable", 
                                  values_to = "Value")
ggplot(data_variances, aes(x = Value)) +
  geom_density(fill = "skyblue", color = "black") +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the variable variances")
```

Similarly, the standard errors are also right-skewed. They could be fixed with a log-transformation, as standard errors are by definition \> 0 (assuming there is variation).

```{r}
#| fig-width: 9
#| fig-height: 7
ggplot(data_variances, aes(x = log(Value))) +
  geom_density(fill = "red", color = "black", alpha = 0.5) +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the log transformations of the variable variances")
```

```{r}
#| fig-width: 9
#| fig-height: 7
# visualize highest values
data_worst = pivot_longer(data %>% select(23:32), 
                                  everything(), 
                                  names_to = "Variable", 
                                  values_to = "Value")
ggplot(data_worst, aes(x = Value)) +
  geom_density(fill = "skyblue", color = "black") +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the highest values of the variables")
```

```{r}
#| fig-width: 9
#| fig-height: 7
ggplot(data_worst, aes(x = sqrt(Value))) +
  geom_density(fill = "red", color = "black", alpha = 0.5) +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the square root transformation of the highest values of the variables")
```

```{r}
#| fig-width: 9
#| fig-height: 7
plot_intro(data)
plot_correlation(data)
```

```{r}
# compute descriptive statistics
summary(data)

# compute share of malignant cancers
malignant_share = sum(data$diagnosis == "M") / nrow(data)
print(paste("The share of malignant cancers in the dataset is ", round(malignant_share, 3), ".", sep = ""))
```

## 2. Data Preprocessing

Text

```{r}
# handle categorical variables appropriately, if present

# change diagnosis to binary variable for easier interpretation
data = data %>% 
  mutate(malignant = as.factor(if_else(diagnosis == "M", TRUE, FALSE))) %>%
  select(-diagnosis) %>%
  relocate(malignant, .before = 2)
```

```{r}
# perform variable selection, if required

# remove id column
data = data %>% select(-id)

# rename variables with whitespaces
colnames(data) = sub(" ", "_", colnames(data))

data = as.data.frame(data, names = TRUE)
```

```{r}
# correlation analysis

# calculate correlation matrix
correlation_matrix <- cor(data[,2:31])
# correlation_matrix

# find attributes that are highly corrected (ideally >0.75)
highly_correlated <- findCorrelation(correlation_matrix, cutoff=0.5)
# highly_correlated
```

As seen above in step 1.4) we should most likely reduce the number of variables used for our classifier. This fits the results presented by the heat map of all variables

```{r}
#| fig-width: 9
#| fig-height: 7
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)

# train the model
model <- train(malignant~., data=data, method="lvq", preProcess="scale", 
                            trControl=control)

# estimate variable importance
importance <- varImp(model, scale=FALSE)

# summarize importance
importance

# plot importance
plot(importance)
```

```{r}
# standardize features
standardize_col <- function(col) {
    return(col - mean(col)) / sd(col)
  }

data = data %>%
  mutate_at(vars(-malignant), ~ standardize_col(.))
```

```{r}
# create training and testing sets
set.seed(-12)
train_size = floor(nrow(data) * 0.8)
train_index = sample(1:nrow(data), train_size, replace = FALSE)
test_index = setdiff(1:nrow(data), train_index)

# prepare as dataframes
train_data = data[train_index, ]
test_data = data[test_index, ]

# prepare matrixes and vectors
train_data_x = train_data %>% select(-malignant) %>% as.matrix()
train_data_y = train_data %>% mutate(malignant = as.logical(malignant)) %>% pull(malignant)
test_data_x = test_data %>% select(-malignant) %>% as.matrix()
test_data_y = test_data %>% mutate(malignant = as.logical(malignant)) %>% pull(malignant)

print(paste("The training set contains", nrow(train_data), "entries."))
print(paste("The testing set contains", nrow(test_data), "entries."))
```

## 3. Model Selection & Model Evaluation

### 3.1 K-Nearest Neighbors

Text

```{r}
# choose model parameters (K) using cross validation
set.seed(-12)

n = nrow(train_data_x)
K_max = floor(nrow(train_data_x) * 0.25) # performance will most likely not improve by defining K above 25% of the dataset 
nfolds = 10  # chosen to minimize variance while keeping bias low
permutation = sample(1:n)  
MSE_fold = matrix(0, nfolds, K_max)  
accuracy_fold = matrix(0, nfolds, K_max) 
recall_fold = matrix(0, nfolds, K_max) 
precision_fold = matrix(0, nfolds, K_max) 

# getting percentage correct and 
for (j in 1:nfolds) {
    test_index = permutation[floor((j-1)*n/nfolds+1) : floor(j*n/nfolds)]  
    train_index = setdiff(1:n, test_index) 
    for (K in 1:K_max) {
        y_hat = knn(train = train_data_x[train_index, ], 
                    test = train_data_x[test_index, ], 
                    cl = train_data_y[train_index], 
                    k = K)
        y_hat = as.logical(y_hat)
        MSE_fold[j,K] = mean((train_data_y[test_index] - y_hat)^2)  
        accuracy_fold[j,K] = mean((train_data_y[test_index] == y_hat))
        recall_fold[j, K] = sum((train_data_y[test_index] == y_hat) & (y_hat == TRUE)) / sum((train_data_y[test_index] == TRUE))
        precision_fold[j, K] = sum((train_data_y[test_index] == y_hat) & (y_hat == TRUE)) / sum((y_hat == TRUE))
    }
}
MSE_cv = colMeans(MSE_fold)
accuracy = colMeans(accuracy_fold)
recall = colMeans(recall_fold)
precision = colMeans(precision_fold)
knn_evaluation = tibble(k = 1:K_max, accuracy, precision, recall)
```

```{r}
#| fig-width: 9
#| fig-height: 7
# visualize results for different K:
ggplot(knn_evaluation, aes(x = k)) +
  geom_line(aes(y = accuracy), 
            color = "blue") +
  geom_line(aes(y = precision), 
            color = "red") +
  geom_line(aes(y = recall), 
            color = "green") + 
  geom_point(mapping = aes(x = which.max(accuracy), y = max(accuracy)), 
             color = "blue", size = 3) + 
  geom_point(mapping = aes(x = which.max(precision), y = max(precision)), 
             color = "red", size = 3) + 
  geom_point(mapping = aes(x = which.max(recall), y = max(recall)), 
             color = "green", size = 3) + 
  geom_text(mapping = aes(x = which.max(accuracy), y = max(accuracy), 
            label = which.max(accuracy)), 
            vjust = 1, hjust = 0) + 
  geom_text(mapping = aes(x = which.max(precision), y = max(precision), 
            label = which.max(precision)), 
            vjust = 1, hjust = 0) + 
  geom_text(mapping = aes(x = which.max(recall), y = max(recall), 
            label = which.max(recall)), 
            vjust = 1, hjust = 0)
```

### 3.2 Random Forest

```{r}
# set up model on training data
RF_model <- randomForest(
  formula = malignant ~ .,
  data = train_data
)

RF_model
```

Looks pretty good! Train data set model accuracy is roughly 96,26%. Go ahead to prediction and confusion matrix

```{r}
pred_RF_train <- predict(RF_model, train_data)
confusionMatrix(pred_RF_train, train_data$malignant)
```

Mini analysis:

```{r}
#| fig-width: 9
#| fig-height: 7
# visualize the tree size 
hist(treesize(RF_model),
     main = "Nodes used for tree construction",
     col = "palegreen")

# again, look at importance of predictors
varImpPlot(RF_model,
           sort = T,
           n.var = 30,
           main = "Importance of predictors")

importance(RF_model)
```

When looking at the 2nd plot depicting the importance of the individual predictors, it becomes apperant that the RF has dealt with the groups of highly correlated variables (via Gini Impurity/Information Gain) and picked the most influential predictor per group for splitting.

```{r}
# prediction for test data
# pred_RF_test <- predict(RF_model, test_data)
# confusionMatrix(pred_RF_test, test_data$malignant)
```

Setup of cross-validation

```{r}
nfolds = 10

mse_fold <- numeric(nfolds)
accuracy_fold <- numeric(nfolds)
precision_fold <- numeric(nfolds)
recall_fold <- numeric(nfolds)
f1_fold <- numeric(nfolds)

# Prepare train/test splits for Cross Validation
set.seed(-12)
permutation = sample(1:nrow(train_data))

# perform Cross Validation

for (j in 1:nfolds){
  test_indices <- permutation[((j - 1) * nrow(train_data) / nfolds + 1) : (j * nrow(train_data) / nfolds)]
  train_indices <- setdiff(1:nrow(train_data), test_indices)

  # obtain training and testing folds
  train_fold <- train_data[train_indices, ]
  test_fold <- train_data[test_indices, ]
  
  # fit Random Forest model on training fold
  RF_model <- randomForest(
    formula = malignant ~ .,
    data = train_fold
  )
  
  # predict on pseudo-test folds
  pred <- predict(RF_model, test_fold)
  
  # obtain MSE
   mse_fold[j] <- mean((as.numeric(pred) - as.numeric(test_fold$malignant))^2)
  
  # obtain evaluation metrics for the fold
  cm <- confusionMatrix(data = pred, reference = test_fold$malignant)
  accuracy_fold[j] <- cm$overall["Accuracy"]
  precision_fold[j] <- cm$byClass["Precision"][1]  # precision for class 'M'
  recall_fold[j] <- cm$byClass["Recall"][1]  # recall for class 'M'
  f1_fold[j] <- cm$byClass["F1"][1]  # F1 Score for class 'M'
}

# compute mean of evaluation metrics across folds
mean_mse <- mean(mse_fold)
mean_accuracy <- mean(accuracy_fold)
mean_precision <- mean(precision_fold)
mean_recall <- mean(recall_fold)
mean_f1 <- mean(f1_fold)

cat("Mean Squared Error:", mean_mse, "\n")
cat("Mean Accuracy:", mean_accuracy, "\n")
cat("Mean Precision:", mean_precision, "\n")
cat("Mean Recall:", mean_recall, "\n")
cat("Mean F1 Score:", mean_f1, "\n")
```

## 4. Fine-Tuning

We fine-tune the Random Forest using grid-search, this time with more options for RF hyperparameters, which are:

-   ntree: The number of generated trees
-   ntry: The number of variables randomly sampled as candidates per split
-   nodesize: The number of terminal nodes

```{r}
# Define the parameter grid
param_grid <- expand.grid(
  mtry = c(2, 4, 6),  # Example values for mtry
  ntree = c(100, 200, 300)  # Example values for ntree
  # Add other parameters to tune if needed
)
```

```{r}
# Perform Cross-Validation
set.seed(123)  # for reproducibility
folds <- createFolds(train_data$malignant, k = nfolds)

# Initialize variables to store results
results <- data.frame(accuracy = numeric(nrow(param_grid)),
                      precision = numeric(nrow(param_grid)),
                      recall = numeric(nrow(param_grid)),
                      f1 = numeric(nrow(param_grid)))

```

```{r}
# Grid Search
for (i in seq(nrow(param_grid))) {
  # Extract hyperparameters
  mtry <- param_grid$mtry[i]
  ntree <- param_grid$ntree[i]
  
  # Perform cross-validation
  for (j in 1:nfolds) {
    train_indices <- setdiff(1:nrow(train_data), folds[[j]])
    test_indices <- folds[[j]]
    
    # Fit model
    RF_model <- randomForest(
      formula = malignant ~ .,
      data = train_data[train_indices, ],
      mtry = mtry,
      ntree = ntree,
      improve = TRUE,
      trace = TRUE
    )
    
    # Predict on test fold
    pred <- predict(RF_model, newdata = train_data[test_indices, ])
    
    # Evaluate performance
    cm <- confusionMatrix(data = pred, reference = train_data$malignant[test_indices])
    results[i, "accuracy"] <- results[i, "accuracy"] + cm$overall["Accuracy"]
    results[i, "precision"] <- results[i, "precision"] + cm$byClass["Precision"][1]
    results[i, "recall"] <- results[i, "recall"] + cm$byClass["Recall"][1]
    results[i, "f1"] <- results[i, "f1"] + cm$byClass["F1"][1]
  }
}

```

```{r}
# Compute mean performance metrics across folds
results <- results / nfolds

# Find the best combination of hyperparameters
best_index <- which.max(results$f1)  # or use any other metric of interest

# Extract the best hyperparameters
best_mtry <- param_grid$mtry[best_index]
best_ntree <- param_grid$ntree[best_index]

```

```{r}
# Train the final model with the best hyperparameters
final_RF_model <- randomForest(
  formula = malignant ~ .,
  data = train_data,
  mtry = best_mtry,
  ntree = best_ntree
)
```

## 5. Final Model Selection

\*\*Advantages of Random Forests:\*\* Works well with small sample sizes and issues with high correlations between predictors. Additionally, they are less prone to overfitting.

\*\*Disadvantages of Random Forests:\*\* Loss of interpretability

## 6. Test Phase

Run fine-tuned Random Forest on test data

**NOTE:** This is the final prediction on the test data already!!

```{r}
# predict ^ obtain predictions as probabilities
pred_test <- predict(final_RF_model, newdata = test_data)
pred_test_prob <- predict(final_RF_model, newdata = test_data, type = "prob")


# obtain final evaluation metrics
final_cm <- confusionMatrix(data = pred_test, reference = test_data$malignant)
test_f1 <- final_cm$byClass["F1"][1]
test_accuracy <- final_cm$overall["Accuracy"]
test_precision <- final_cm$byClass["Precision"][1]
test_recall <- final_cm$byClass["Sensitivity"][1]
test_specificity <- final_cm$byClass["Specificity"][1]

test_mse  <- 1 - test_accuracy

# print final F1 score
cat("Final F1 Score:", test_f1, "\n")
cat("Final Accuracy:", test_accuracy, "\n")
cat("Final Precision:", test_precision, "\n")
cat("Final Recall:", test_recall, "\n")
cat("Final MSE:", test_mse, "\n")
```

And finally, compute some Analytics and visualize!!

```{r}
#| fig-width: 9
#| fig-height: 7

library(PRROC)

# Predict probabilities on the test set
pred_prob <- predict(final_RF_model, newdata = test_data, type = "prob")

# Extract probabilities for the positive class ('TRUE' class)
pred_prob_true <- pred_prob[, "TRUE"]

# Calculate precision and recall
pr_values <- pr.curve(scores.class0 = pred_prob_true, weights.class0 = ifelse(test_data$malignant == "TRUE", 1, 0), curve=TRUE)

plot(pr_values)

# Compute ROC curve
roc_values <- roc.curve(scores.class0 = pred_prob_true, weights.class0 = ifelse(test_data$malignant == "TRUE", 1, 0), curve = TRUE)

plot(roc_values)
```

## 7. Appendix

Let's check if we obtain the same results after selecting only the most important variables!

```{r}
# subset data
data = data %>% select(malignant, perimeter_worst, perimeter_mean, perimeter_se, concave_points_se, smoothness_worst, fractal_dimension_se, symmetry_se, smoothness_se, fractal_dimension_mean, texture_se)

# split into training an test data
set.seed(-12)
train_size = floor(nrow(data) * 0.8)
train_index = sample(1:nrow(data), train_size, replace = FALSE)
test_index = setdiff(1:nrow(data), train_index)

# prepare as dataframes
train_data_subset = data[train_index, ]
test_data_subset = data[test_index, ]

```

Train another RandomForest using the optimal hyperparameters revieled above:

```{r}
# Train the final model with the best hyperparameters
final_RF_model <- randomForest(
  formula = malignant ~ .,
  data = train_data_subset,
  mtry = best_mtry,
  ntree = best_ntree
)

# and obtain predictions

# predict ^ obtain predictions as probabilities
pred_test_subset <- predict(final_RF_model, newdata = test_data_subset)
pred_test_prob_subset <- predict(final_RF_model, newdata = test_data_subset, type = "prob")


# obtain final evaluation metrics
final_cm_subset <- confusionMatrix(data = pred_test_subset, reference = test_data$malignant)
test_f1_subset <- final_cm_subset$byClass["F1"][1]
test_accuracy_subset <- final_cm_subset$overall["Accuracy"]
test_precision_subset <- final_cm_subset$byClass["Precision"][1]
test_recall_subset <- final_cm_subset$byClass["Sensitivity"][1]
test_specificity_subset <- final_cm_subset$byClass["Specificity"][1]

test_mse_subset  <- 1 - test_accuracy

# print final F1 score
cat("Final F1 Score:", test_f1_subset, "\n")
cat("Final Accuracy:", test_accuracy_subset, "\n")
cat("Final Precision:", test_precision_subset, "\n")
cat("Final Recall:", test_recall_subset, "\n")
cat("Final MSE:", test_mse_subset, "\n")

```

Performance decreases - it is thus adviced to let the random forest compute the Gini Impurities and let it sort through the different variables itself than us 'prepruning' it by limiting its selection of variables.
