---
title: "joerdis_project_wip"
author: "Joerdis Strack"
format: html
editor: visual
---

# WIP Final Project - Random Forests

Load relevant libs

```{r}
# libs

library(readr)
library(GGally) # <- for pair plots
library(dplyr)
library(mlbench) # <- feature selection
library(caret) # <- feature selection
library(DataExplorer)
library(randomForest)
```

```{r}
# set working directory
setwd("C:/Users/Joerd/Desktop/statistical_learning/final_project")

# read in data
data <- read_csv("C:/Users/Joerd/Desktop/statistical_learning/final_project/BreastCancer.csv")

```

# 1) Data Exploration

## 1.1) Explore the features and labels:

obtain feature names, data type and look at first 6 rows of data

```{r}
head(data)
str(data)
dim(data)
```

The dataset contains 32 variables, of which 'diagnosis' holds the ground truth class labels and is of type character. The other variables are numerical and appear to capture info about various physical measures of patients. I do not know, what ...33 stands for - it is of type logial/boolean. There are 568 rows of data.

## 1.2) Check for missing values and handle them appropriately if necessary:

```{r}

# count missing values per column
missing_count_per_column <- colSums(is.na(data))
missing_count_per_column

# INTERPRETATION: There are only missing values for variable
# data$...33 -> is this the 'empty' class variable we are meant to 
# predict?

data <- subset(data, select = -c(...33))
data <- subset(data, select = -c(1)) #<- drop id

# again, check missing values per column
# count missing values per column
missing_count_per_column_after <- colSums(is.na(data))
missing_count_per_column_after

# successful!

```

## 1.3) Visualize the distribution of classes and features:

First, obtain a pairs plot

```{r}
ggpairs(data)
```

Interpretation: While there is obviously too much info, we can see that the data follow a bell-shaped distributed and that many variables appear to be positively skewed.

```{r}
plot_intro(data)
plot_bar(data)
plot_correlation(data)
```

INTERPRETATION: Looks like we have many variables that are highly correlated with each other

## 1.4) Compute descriptive statistics for the dataset:

```{r}
summary(data)
```

# 2) Data Preprocessing

## 2.3) Handle any categorical variables appropriately (if present):

```{r}
data$diagnosis <- as.factor(data$diagnosis) # <- nec for RF

# transform tibble into dataframe
df <- as.data.frame(data, names=TRUE)
df_copy <- subset(df, select = -c(1)) # <- drop diagnosis for convenience 
```

## 2.2) Standardize or normalize the features if required:

```{r}
df_standardized <- df_copy %>% mutate_all(~(scale(.) %>% as.vector))

# check
# summary(df_standardized)

# reinclude 'diagnosis'
df_standardized <- mutate(df_standardized, 
                          diagnosis = df$diagnosis)

# check
# summary(df_standardized)
```

## 2.1) Split the dataset into training and testing sets:

```{r}
set.seed(-12)

train_index = sample(nrow(df_standardized), 
                     size = trunc(0.8 * nrow(df_standardized)))
# NOTE: I have opted for an 80/20 split, but we can do whatever


# replace empty spaces with underscore
colnames(df_standardized) <- sub(" ", "_", colnames(df_standardized))

train_data = df_standardized[train_index, ]
test_data = df_standardized[-train_index, ]

```

## 2.4) Perform variable selection if required:

```{r}
# correlation analysis

# calculate correlation matrix
correlation_matrix <- cor(data[,2:31])
correlation_matrix

# find attributes that are highly corrected (ideally >0.75)
highly_correlated <- findCorrelation(correlation_matrix, cutoff=0.5)
highly_correlated
```

As seen above in step 1.4) we should most likely reduce the number of variables used for our classifier. This fits the results presented by the heat map of all variabels

```{r}
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)

# train the model
model <- train(diagnosis~., data=df, method="lvq", preProcess="scale", 
                            trControl=control)

# estimate variable importance
importance <- varImp(model, scale=FALSE)

# summarize importance
importance

# plot importance
plot(importance)
```

INTERPRETATION: All variables appear to be at least moderatly if not highly influential. I thus do not think that we can simply exclude variables from the analysis without seriously distorting our results. However, we might have to consider creating indicators comprising info on highly correlated variables (apparent from heat map and structure of importance plot).

# 3) Model Selection and Model Evaluation:

## 3.1) Choose two models: KNN and RF

## 3.2) Implement RF

```{r}
# set up model on training data
RF_model <- randomForest(
  formula = diagnosis ~ .,
  data = train_data
)

RF_model
```

Looks pretty good! Train data set model accuracy is roughly 96,26%. Go ahead to prediction and confusion matrix

```{r}
pred_RF_train <- predict(RF_model, train_data)
confusionMatrix(pred_RF_train, train_data$diagnosis)
```

Mini analysis:

```{r}
# visualize the tree size 
hist(treesize(RF_model),
     main = "Nodes used for tree construction",
     col = "palegreen")

# again, look at importance of predictors
varImpPlot(RF_model,
           sort = T,
           n.var = 30,
           main = "Importance of predictors")

importance(RF_model)
```

Wow, this looks so different!!

```{r}
# prediction for test data
#pred_RF_test <- predict(RF_model, test_data)
#confusionMatrix(pred_RF_test, test_data$diagnosis)
```

Setup of cross-validation

```{r}
nfolds = 10

accuracy_fold <- numeric(nfolds)
precision_fold <- numeric(nfolds)
recall_fold <- numeric(nfolds)
f1_fold <- numeric(nfolds)

# Prepare train/test splits for Cross Validation
set.seed(-12)
permutation = sample(1:nrow(train_data))

# perform Cross Validation

for (j in 1:nfolds){
  test_indices <- permutation[((j - 1) * nrow(train_data) / nfolds + 1) : (j * nrow(train_data) / nfolds)]
  train_indices <- setdiff(1:nrow(train_data), test_indices)

  # obtain training and testing folds
  train_fold <- train_data[train_indices, ]
  test_fold <- train_data[test_indices, ]
  
  # fit Random Forest model on training fold
  RF_model <- randomForest(
    formula = diagnosis ~ .,
    data = train_fold
  )
  
  # predict on pseudo-test folds
  pred <- predict(RF_model, test_fold)
  
  # obtain evaluation metrics for the fold
  cm <- confusionMatrix(data = pred, reference = test_fold$diagnosis)
  accuracy_fold[j] <- cm$overall["Accuracy"]
  precision_fold[j] <- cm$byClass["Precision"][1]  # precision for class 'M'
  recall_fold[j] <- cm$byClass["Recall"][1]  # recall for class 'M'
  f1_fold[j] <- cm$byClass["F1"][1]  # F1 Score for class 'M'
}

# compute mean of evaluation metrics across folds
mean_accuracy <- mean(accuracy_fold)
mean_precision <- mean(precision_fold)
mean_recall <- mean(recall_fold)
mean_f1 <- mean(f1_fold)

cat("Mean Accuracy:", mean_accuracy, "\n")
cat("Mean Precision:", mean_precision, "\n")
cat("Mean Recall:", mean_recall, "\n")
cat("Mean F1 Score:", mean_f1, "\n")
```

# 5) Fine-tuning

```{r}
tuned_RF_model <- tuneRF(train_data[,-5], train_data[,5],
            stepFactor = 0.5,
            plot = TRUE,
            ntreeTry = 110, # <- I've played around with it & 110 seems to be highest
            trace = TRUE,
            improve = 0.05)
```
