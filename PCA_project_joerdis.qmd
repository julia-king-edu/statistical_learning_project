---
title: "Final Project"
author: "JÃ¶rdis Strack, Julia King"
format: html
editor: source
---

## 0. Setup Chunk

Text

```{r}
#| output: false
if (!require("pacman")) install.packages("pacman")
pacman::p_load(readr, tidyverse, class, caret, bench, DataExplorer, randomForest)
```

## 1. Data Exploration

Text

```{r}
#| warning: false
# import dataset
data = read_csv("data/BreastCancer.csv", show_col_types = FALSE)
```

```{r}
# first overview
head(data)
```

```{r}
# explore features and labels
str(data)
```

```{r}
# continue exploration
dim(data)
```

```{r}
# count missing values per column
missing_count_per_column <- colSums(is.na(data))
missing_count_per_column

# INTERPRETATION: There are only missing values for variable
# data$...33 -> caused by trailing comma in csv

data <- subset(data, select = -c(...33))
```

```{r}
#| fig-width: 9
#| fig-height: 7
# visualize distribution of classes

# visualize means
data_means = pivot_longer(data %>% select(3:12), 
                                  everything(), 
                                  names_to = "Variable", 
                                  values_to = "Value")
ggplot(data_means, aes(x = Value)) +
  geom_density(fill = "skyblue", color = "black", alpha = 0.5) +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the variable means")
```

Most means seem to be right-skewed. Thus, they will likely require some form of correction. Taking the logarithm is infeasible because of 0 values. Thus, square roots are shown below:

```{r}
#| fig-width: 9
#| fig-height: 7
ggplot(data_means, aes(x = sqrt(Value))) +
  geom_density(fill = "red", color = "black", alpha = 0.5) +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the square root transformation of the variable means")
```

```{r}
#| fig-width: 9
#| fig-height: 7
# visualize standard errors
data_variances = pivot_longer(data %>% select(13:22), 
                                  everything(), 
                                  names_to = "Variable", 
                                  values_to = "Value")
ggplot(data_variances, aes(x = Value)) +
  geom_density(fill = "skyblue", color = "black") +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the variable variances")
```

Similarly, the standard errors are also right-skewed. They could be fixed with a log-transformation, as standard errors are by definition \> 0 (assuming there is variation).

```{r}
#| fig-width: 9
#| fig-height: 7
ggplot(data_variances, aes(x = log(Value))) +
  geom_density(fill = "red", color = "black", alpha = 0.5) +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the log transformations of the variable variances")
```

```{r}
#| fig-width: 9
#| fig-height: 7
# visualize highest values
data_worst = pivot_longer(data %>% select(23:32), 
                                  everything(), 
                                  names_to = "Variable", 
                                  values_to = "Value")
ggplot(data_worst, aes(x = Value)) +
  geom_density(fill = "skyblue", color = "black") +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the highest values of the variables")
```

```{r}
#| fig-width: 9
#| fig-height: 7
ggplot(data_worst, aes(x = sqrt(Value))) +
  geom_density(fill = "red", color = "black", alpha = 0.5) +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the square root transformation of the highest values of the variables")
```

```{r}
#| fig-width: 9
#| fig-height: 7
plot_intro(data)
plot_correlation(data)
```

```{r}
# compute descriptive statistics
summary(data)

# compute share of malignant cancers
malignant_share = sum(data$diagnosis == "M") / nrow(data)
print(paste("The share of malignant cancers in the dataset is ", round(malignant_share, 3), ".", sep = ""))
```

## 2. Data Preprocessing

Text

```{r}
# handle categorical variables appropriately, if present

# change diagnosis to binary variable for easier interpretation
data = data %>% 
  mutate(malignant = as.factor(if_else(diagnosis == "M", TRUE, FALSE))) %>%
  select(-diagnosis) %>%
  relocate(malignant, .before = 2)
```

```{r}
# perform variable selection, if required

# remove id column
data = data %>% select(-id)

# rename variables with whitespaces
colnames(data) = sub(" ", "_", colnames(data))

data = as.data.frame(data, names = TRUE)
```

```{r}
# correlation analysis

# calculate correlation matrix
# correlation_matrix <- cor(data[,2:31])
# correlation_matrix

# find attributes that are highly corrected (ideally >0.75)
# highly_correlated <- findCorrelation(correlation_matrix, cutoff=0.5)
# highly_correlated
```

As seen above in step 1.4) we should most likely reduce the number of variables used for our classifier. This fits the results presented by the heat map of all variabels

```{r}
#| fig-width: 9
#| fig-height: 7
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)

# train the model
model <- train(malignant~., data=data, method="lvq", preProcess="scale", 
                            trControl=control)

# estimate variable importance
importance <- varImp(model, scale=FALSE)

# summarize importance
importance

# plot importance
plot(importance)
```

```{r}
# standardize features

data = data %>%
  mutate_at(vars(-malignant), ~ scale(.))
```

```{r}
# create training and testing sets
set.seed(-12)
train_size = floor(nrow(data) * 0.8)
train_index = sample(1:nrow(data), train_size, replace = FALSE)
test_index = setdiff(1:nrow(data), train_index)

# prepare as dataframes
train_data = data[train_index, ]
test_data = data[test_index, ]

# prepare matrixes and vectors
train_data_x = train_data %>% select(-malignant) %>% as.matrix()
train_data_y = train_data %>% mutate(malignant = as.logical(malignant)) %>% pull(malignant)
test_data_x = test_data %>% select(-malignant) %>% as.matrix()
test_data_y = test_data %>% mutate(malignant = as.logical(malignant)) %>% pull(malignant)

print(paste("The training set contains", nrow(train_data), "entries."))
print(paste("The testing set contains", nrow(test_data), "entries."))
```

```{r}
# prepare pca
pca_obj = prcomp(train_data_x, scale = TRUE)

get_PVE = function(pca_out) {
  pca_out$sdev ^ 2 / sum(pca_out$sdev ^ 2)
}

pca_obj = prcomp(train_data_x, scale = TRUE)
pve = get_PVE(pca_obj)
```

```{r}
# choose cutoff level for percentage of variance explained
pve_percent = 0.9
pca_cutoff = which(cumsum(pve) > pve_percent)[1]

biplot(pca_obj, scale = 0)
```

```{r}
ggplot(mapping = aes(x = 1:length(pve), y = cumsum(pve))) + 
  geom_line() + 
  geom_abline(slope = 0, intercept = pve_percent, linetype = "dashed") + 
  geom_point(mapping = aes(x = pca_cutoff, y = cumsum(pve)[pca_cutoff])) + 
  geom_text(mapping = aes(x = pca_cutoff, y = cumsum(pve)[pca_cutoff], 
                          label = paste("(", pca_cutoff, ", ", round(cumsum(pve)[pca_cutoff], 3), ")", sep = "")), 
            vjust = 1.5, hjust = 0)
```

```{r}
# 
test_data_x_pca = scale(test_data_x) %*% pca_obj$rotation[, 1:pca_cutoff]
train_data_x_pca = pca_obj$x[, 1:pca_cutoff]
```

## 3. Model Selection & Model Evaluation

### 3.1 K-Nearest Neighbors

Text

```{r}
evaluate_KNN <- function(train_x, train_y, test_x, test_y, k_range = 1:10, seed = -12) {
  # check input validity
  if (nrow(train_x) != length(train_y)) {
    stop("Number of elements in classification array train_y does not match number of rows in train_x.")
  }
  if (nrow(test_x) != length(test_y)) {
    stop("Number of elements in classification array test_y does not match number of rows in test_x.")
  }
  if (min(k_range < 1)) {
    stop("Minimum of k_range must be >= 1.")
  }
  if (nrow(train_x) <= max(k_range)) {
    stop("Maximum of k_range is too large for specified nr. of splits and dataset size.")
  }
  # define variables
  mse = c()
  accuracy = c()
  precision = c()
  recall = c()
  n = nrow(test_x)
  
  # run knn & compute prediction matrix
  for (k in k_range) {
    test_y_hat = knn(train = train_x, 
                     test = test_x, 
                     cl = train_y, 
                     k = k)
    test_y_hat = as.logical(test_y_hat)
    pred_matrix = table(Actual = test_y, Predicted = test_y_hat) # columns = predicted, rows = actual, 1 for false, 2 for true. EX: pred_matrix[1, 2] returns false positives
    
    # compute evaluation criteria
    mse = c(mse, mean((test_y - test_y_hat)^2))
    accuracy = c(accuracy, (pred_matrix[1, 1] + pred_matrix[2, 2]) / n)
    precision = c(precision, (pred_matrix[2, 2]) / (pred_matrix[2, 2] + pred_matrix[1, 2]))
    recall = c(recall, (pred_matrix[2, 2]) / (pred_matrix[2, 2] + pred_matrix[2, 1]))
  }
  
  # combine results & return 
  results = data.frame(k = k_range, mse, accuracy, precision, recall)
  return(results)
}
```

```{r}
cross_validate_knn <- function(x, y, n_splits = 10, k_range = 1:10, seed = -12) {
  # validate inputs
  if ((nrow(x) != length(y)) | (length(y) == 0)) {
    stop("Number of elements in classification array train_y does not match number of rows in train_x or is empty.")
  }
  if (n_splits < 1) {
    stop("Number of splits must be larger than 0.")
  }
  if (min(k_range < 1)) {
    stop("Minimum of k_range must be >= 1.")
  }
  if (floor(nrow(x) * (1 - (1 / n_splits))) <= max(k_range)) { # k_range exceeds size of training set in one or more of the splits
    stop("Maximum of k_range is too large for specified nr. of splits and dataset size.")
  }
  
  # initialize variables
  len = nrow(x)
  result = data.frame(k = numeric(), 
                      mse = numeric(), 
                      accuracy = numeric(), 
                      precision = numeric(), 
                      recall = numeric())
  
  # shuffle dataset (index)
  set.seed(seed)
  indexes = sample(1:len)
  
  for (i in 1:n_splits) {
    # split dataset indexes 
    test_index = indexes[floor((i-1)*len/n_splits+1) : floor(i*len/n_splits)]  
    train_index = setdiff(1:len, test_index) 
    # run evaluate_knn
    current_results = evaluate_KNN(train_x = x[train_index, ], train_y = y[train_index], 
                                   test_x = x[test_index, ], test_y = y[test_index], 
                                   k_range = k_range)
    # combine dataframes
    result = rbind(result, current_results)
  }
  # calculate averages and return 
  result = result %>% 
    group_by(k) %>% 
    summarise(across(everything(), mean))
  return(result)
}
```

```{r}
# run cross-validation
knn_eval_cv = cross_validate_knn(train_data_x, train_data_y, 
                                    n_splits = 10, k_range = 1:(nrow(train_data_x) %/% 3))
```



```{r}
#| fig-width: 9
#| fig-height: 7
# visualize results for different K:
ggplot(knn_eval_cv, aes(x = k)) +
  geom_line(aes(y = accuracy, color = "Accuracy")) +
  geom_line(aes(y = precision, color = "Precision")) +
  geom_line(aes(y = recall, color = "Recall")) + 
  # add points where the metrics are maiximized
  geom_point(aes(x = k[which.max(accuracy)], y = max(accuracy), color = "Accuracy"), size = 3) + 
  geom_point(aes(x = k[which.max(precision)], y = max(precision), color = "Precision"), size = 3) + 
  geom_point(aes(x = k[which.max(recall)], y = max(recall), color = "Recall"), size = 3) + 
  # add text to the points indicating the k required to reach said maximum
  geom_text(aes(x = which.max(accuracy), y = max(accuracy), 
                label = paste("(", k[which.max(accuracy)], ",", round(max(accuracy), 3), ")", sep = "")), 
            vjust = 2, hjust = 0) + 
  geom_text(aes(x = which.max(precision), y = max(precision), 
                label = paste("(", k[which.max(precision)], ",", round(max(precision), 3), ")", sep = "")), 
            vjust = 1.5, hjust = 0) + 
  geom_text(aes(x = which.max(recall), y = max(recall), 
                label = paste("(", k[which.max(recall)], ",", round(max(recall), 3), ")", sep = "")), 
            vjust = 1.5, hjust = 0) + 
  # add titles, legend & colors
  labs(title = "Cross-validation results of KNN on training data",
       x = "k",
       y = "score",
       color = "Metric") +
  scale_color_manual(values = c("Accuracy" = "blue", "Precision" = "red", "Recall" = "green"),
                     labels = c("Accuracy", "Precision", "Recall"))
```

# todo: visualize pca results

### 3.2 Random Forest

```{r}
# set up model on training data
RF_model <- randomForest(
  formula = malignant ~ .,
  data = train_data
)

RF_model
```

Training data set model accuracy is roughly 96,26%. Go ahead to prediction and confusion matrix

```{r}
pred_RF_train <- predict(RF_model, train_data)
confusionMatrix(pred_RF_train, train_data$malignant)
```

Mini analysis:

```{r}
#| fig-width: 9
#| fig-height: 7
# visualize the tree size 
hist(treesize(RF_model),
     main = "Nodes used for tree construction",
     col = "palegreen")

# again, look at importance of predictors
varImpPlot(RF_model,
           sort = T,
           n.var = 30,
           main = "Importance of predictors")

importance(RF_model)
```

Put nice comment discussing the differences between the trees.

Now, run Random Forest over the PCA version of the data for comparison.

```{r}
# convert train_data_x_pca into data frame
train_data_x_pca_DF <- as.data.frame(train_data_x_pca)
test_data_x_pca_DF <- as.data.frame(test_data_x_pca)

# add y = malignant
train_data_pca_RF <- cbind(train_data_x_pca_DF, malignant = train_data_y)
test_data_pca_RF <- cbind(test_data_x_pca_DF, malignant = test_data_y)

train_data_pca_RF$malignant <- train_data_y
test_data_pca_RF$malignant <- test_data_y

# Fit Random Forest model on PCA-transformed data
RF_model_PCA <- randomForest(
  formula = malignant ~ .,
  data = train_data_pca_RF
)
RF_model_PCA

# Predict on training data
pred_RF_train_PCA <- predict(RF_model_PCA, train_data_pca_RF, type='response')

preds = ifelse(pred_RF_train_PCA > 0.5, "TRUE", "FALSE")
preds = as.factor(preds)

confusionMatrix(preds, as.factor(train_data_pca_RF$malignant))
```
Run on PCA test data

```{r}
# Fit Random Forest model on PCA-transformed data
RF_model_PCA_test <- randomForest(
  formula = malignant ~ .,
  data = test_data_pca_RF
)
RF_model_PCA_test

# Predict on training data
pred_RF_test_PCA <- predict(RF_model_PCA_test, test_data_pca_RF, type='response')

preds_test = ifelse(pred_RF_test_PCA > 0.5, "TRUE", "FALSE")
preds_test = as.factor(preds_test)

PCA_conf_RF <- confusionMatrix(preds_test, as.factor(test_data_pca_RF$malignant))
PCA_conf_RF
```

And of course, for comparison a small model analysis of the Random Forest.

```{r}
#| fig-width: 9
#| fig-height: 7
# visualize the tree size 
hist(treesize(RF_model_PCA),
     main = "Nodes used for tree construction",
     col = "palegreen")

# again, look at importance of predictors
varImpPlot(RF_model_PCA,
           sort = T,
           n.var = 30,
           main = "Importance of predictors")

importance(RF_model_PCA)
```
Finally, obtain performance metrics for PCA test data:

```{r}
accuracy <- PCA_conf_RF$overall['Accuracy']
F1_score <- PCA_conf_RF$byClass['F1']
recall <- PCA_conf_RF$byClass['Sensitivity']

# metrics
cat("Accuracy:", accuracy, "\n")
cat("F1 Score:", F1_score, "\n")
cat("Recall (Sensitivity):", recall, "\n")
```


Setup of cross-validation

```{r}
nfolds = 10

accuracy_fold <- numeric(nfolds)
precision_fold <- numeric(nfolds)
recall_fold <- numeric(nfolds)
f1_fold <- numeric(nfolds)

accuracy_fold_PCA <- numeric(nfolds)
precision_fold_PCA <- numeric(nfolds)
recall_fold_PCA <- numeric(nfolds)
f1_fold_PCA <- numeric(nfolds)

# Prepare train/test splits for Cross Validation
set.seed(-12)
permutation <- sample(1:nrow(train_data))

# perform Cross Validation

for (j in 1:nfolds){
  test_indices <- permutation[((j - 1) * nrow(train_data) / nfolds + 1) : (j * nrow(train_data) / nfolds)]
  train_indices <- setdiff(1:nrow(train_data), test_indices)

  # obtain training and testing folds
  train_fold <- train_data[train_indices, ]
  test_fold <- train_data[test_indices, ]
  
  train_fold_PCA <- train_data_pca_RF[train_indices, ]
  test_fold_PCA <- train_data_pca_RF[test_indices, ]
  
  # fit Random Forest model on training fold
  RF_model <- randomForest(
    formula = malignant ~ .,
    data = train_fold
  )
  
  ### NOTE: I am suppressing warnings here, since RF (especially in combination with PCA) makes use of regression and the reduction of features to fewer principle components might affect the model structurally and could cause overfitting. We are aware of this and consider it in the analysis but it makes the output look a bit convoluted!
  
  suppressWarnings({
  # RF_model_PCA
  RF_model_PCA <- randomForest(
  formula = malignant ~ .,
  data = train_fold_PCA
  )})
  
  # predict on pseudo-test folds
  pred <- predict(RF_model, test_fold)
  
  # Predict on training data
  pred_RF_test_PCA <- predict(RF_model_PCA, test_fold_PCA, type='response')
  
  preds_test = ifelse(pred_RF_test_PCA > 0.5, "TRUE",  "FALSE")
  preds_test = as.factor(preds_test)

  # obtain evaluation metrics for the fold
  cm <- confusionMatrix(data = pred, reference = test_fold$malignant)
  accuracy_fold[j] <- cm$overall["Accuracy"]
  precision_fold[j] <- cm$byClass["Precision"][1]  # precision for class 'M'
  recall_fold[j] <- cm$byClass["Recall"][1]  # recall for class 'M'
  f1_fold[j] <- cm$byClass["F1"][1]  # F1 Score for class 'M'

  # obtain confusion matrix for PCA
  cm_PCA <- confusionMatrix(data = preds_test, reference = as.factor(test_fold_PCA$malignant))
  accuracy_fold_PCA[j] <- cm_PCA$overall["Accuracy"]
  precision_fold_PCA[j] <- cm_PCA$byClass["Precision"][1]  # precision for class 'M'
  recall_fold_PCA[j] <- cm_PCA$byClass["Recall"][1]  # recall for class 'M'
  f1_fold_PCA[j] <- cm_PCA$byClass["F1"][1]  # F1 Score for class 'M'
}
```
And compute performance metrics:

```{r}
# compute mean of evaluation metrics across folds
mean_accuracy <- mean(accuracy_fold)
mean_precision <- mean(precision_fold)
mean_recall <- mean(recall_fold)
mean_f1 <- mean(f1_fold)

# compute mean of evaluation metrics across folds
mean_accuracy_pca <- mean(accuracy_fold_PCA)
mean_precision_pca <- mean(precision_fold_PCA)
mean_recall_pca <- mean(recall_fold_PCA)
mean_f1_pca <- mean(f1_fold_PCA)

cat("Mean Accuracy:", mean_accuracy, "\n")
cat("Mean Precision:", mean_precision, "\n")
cat("Mean Recall:", mean_recall, "\n")
cat("Mean F1 Score:", mean_f1, "\n")

cat("\n")

cat("Mean Accuracy PCA:", mean_accuracy_pca, "\n")
cat("Mean Precision PCA:", mean_precision_pca, "\n")
cat("Mean Recall PCA:", mean_recall_pca, "\n")
cat("Mean F1 Score PCA:", mean_f1_pca, "\n")
```
We observe a slight decrease in performance once we run the random forest on PCA data - which makes sense, given that cross-validated results tend to be more precise than performance (MSE estimates) based on just training and test data, which underestimates the true MSE or Misclassification error. Still, we have to consider the potential of overfitting the PCA Random Forest, due to a decrease of 31 (?) features to 6 PCs.

## 4. Fine-Tuning

## 4.1 KNN

# todo

## 4.2 Random Forest

We fine-tune the Random Forest using grid-search, this time with more options for RF hyperparameters, which are:

-   ntree: The number of generated trees
-   ntry: The number of variables randomly sampled as candidates per split

```{r}
# Define the parameter grid
param_grid <- expand.grid(
  mtry = c(2, 4, 6),  # Example values for mtry
  ntree = c(100, 200, 300)  # Example values for ntree
  # Add other parameters to tune if needed
)
```

```{r}
# Perform Cross-Validation
set.seed(123)  # for reproducibility
folds <- createFolds(train_data$malignant, k = nfolds)

# Initialize variables to store results
results <- data.frame(accuracy = numeric(nrow(param_grid)),
                      precision = numeric(nrow(param_grid)),
                      recall = numeric(nrow(param_grid)),
                      f1 = numeric(nrow(param_grid)))

```

```{r}
# Grid Search
for (i in seq(nrow(param_grid))) {
  # Extract hyperparameters
  mtry <- param_grid$mtry[i]
  ntree <- param_grid$ntree[i]
  
  # Perform cross-validation
  for (j in 1:nfolds) {
    train_indices <- setdiff(1:nrow(train_data), folds[[j]])
    test_indices <- folds[[j]]
    
    # Fit model
    RF_model <- randomForest(
      formula = malignant ~ .,
      data = train_data[train_indices, ],
      mtry = mtry,
      ntree = ntree,
      improve = TRUE,
      trace = TRUE
    )
    
    # Predict on test fold
    pred <- predict(RF_model, newdata = train_data[test_indices, ])
    
    # Evaluate performance
    cm <- confusionMatrix(data = pred, reference = train_data$malignant[test_indices])
    results[i, "accuracy"] <- results[i, "accuracy"] + cm$overall["Accuracy"]
    results[i, "precision"] <- results[i, "precision"] + cm$byClass["Precision"][1]
    results[i, "recall"] <- results[i, "recall"] + cm$byClass["Recall"][1]
    results[i, "f1"] <- results[i, "f1"] + cm$byClass["F1"][1]
  }
}

```

```{r}
# Compute mean performance metrics across folds
results <- results / nfolds

# Find the best combination of hyperparameters
best_index <- which.max(results$f1)  # or use any other metric of interest

# Extract the best hyperparameters
best_mtry <- param_grid$mtry[best_index]
best_ntree <- param_grid$ntree[best_index]

```

```{r}
# Train the final model with the best hyperparameters
final_RF_model <- randomForest(
  formula = malignant ~ .,
  data = train_data,
  mtry = best_mtry,
  ntree = best_ntree
)
```

## 5. Final Model Selection

\*\*Advantages of Random Forests:\*\* Works well with small sample sizes and issues with high correlations between predictors. Additionally, they are less prone to overfitting.

\*\*Disadvantages of Random Forests:\*\* Loss of interpretability

## 6. Applying to test dataset

Run fine-tuned Random Forest on test data

**NOTE:** This is the final prediction on the test data already!!

```{r}
# predict ^ obtain predictions as probabilities
pred_test <- predict(final_RF_model, newdata = test_data)
pred_test_prob <- predict(final_RF_model, newdata = test_data, type = "prob")


# obtain final evaluation metrics
final_cm <- confusionMatrix(data = pred_test, reference = test_data$malignant)
test_f1 <- final_cm$byClass["F1"][1]
test_accuracy <- final_cm$overall["Accuracy"]
test_precision <- final_cm$byClass["Precision"][1]
test_recall <- final_cm$byClass["Sensitivity"][1]
test_specificity <- final_cm$byClass["Specificity"][1]

test_mse  <- 1 - test_accuracy

# print final F1 score
cat("Final F1 Score:", test_f1, "\n")
cat("Final Accuracy:", test_accuracy, "\n")
cat("Final Precision:", test_precision, "\n")
cat("Final Recall:", test_recall, "\n")
cat("Final MSE:", test_mse, "\n")
```

And finally, compute some Analytics and visualize!!

```{r}
#| fig-width: 9
#| fig-height: 7

library(PRROC)

# Predict probabilities on the test set
pred_prob <- predict(final_RF_model, newdata = test_data, type = "prob")

# Extract probabilities for the positive class ('TRUE' class)
pred_prob_true <- pred_prob[, "TRUE"]

# Calculate precision and recall
pr_values <- pr.curve(scores.class0 = pred_prob_true, weights.class0 = ifelse(test_data$malignant == "TRUE", 1, 0), curve=TRUE)

plot(pr_values, col='black')

# Compute ROC curve
roc_values <- roc.curve(scores.class0 = pred_prob_true, weights.class0 = ifelse(test_data$malignant == "TRUE", 1, 0), curve = TRUE)

plot(roc_values, col='black')
```

## 7. Appendix

Let's check if we obtain the same results after selecting only the most important variables!

```{r}
# subset data
data = data %>% select(malignant, perimeter_worst, perimeter_mean, perimeter_se, concave_points_se, smoothness_worst, fractal_dimension_se, symmetry_se, smoothness_se, fractal_dimension_mean, texture_se)

# split into training an test data
set.seed(-12)
train_size = floor(nrow(data) * 0.8)
train_index = sample(1:nrow(data), train_size, replace = FALSE)
test_index = setdiff(1:nrow(data), train_index)

# prepare as dataframes
train_data_subset = data[train_index, ]
test_data_subset = data[test_index, ]

```

Train another RandomForest using the optimal hyperparameters revieled above:

```{r}
# Train the final model with the best hyperparameters
final_RF_model <- randomForest(
  formula = malignant ~ .,
  data = train_data_subset,
  mtry = best_mtry,
  ntree = best_ntree
)

# and obtain predictions

# predict ^ obtain predictions as probabilities
pred_test_subset <- predict(final_RF_model, newdata = test_data_subset)
pred_test_prob_subset <- predict(final_RF_model, newdata = test_data_subset, type = "prob")


# obtain final evaluation metrics
final_cm_subset <- confusionMatrix(data = pred_test_subset, reference = test_data$malignant)
test_f1_subset <- final_cm_subset$byClass["F1"][1]
test_accuracy_subset <- final_cm_subset$overall["Accuracy"]
test_precision_subset <- final_cm_subset$byClass["Precision"][1]
test_recall_subset <- final_cm_subset$byClass["Sensitivity"][1]
test_specificity_subset <- final_cm_subset$byClass["Specificity"][1]

test_mse_subset  <- 1 - test_accuracy

# print final F1 score
cat("Final F1 Score:", test_f1_subset, "\n")
cat("Final Accuracy:", test_accuracy_subset, "\n")
cat("Final Precision:", test_precision_subset, "\n")
cat("Final Recall:", test_recall_subset, "\n")
cat("Final MSE:", test_mse_subset, "\n")

```

Performance decreases - it is thus adviced to let the random forest compute the Gini Impurities and let it sort through the different variables itself than us 'prepruning' it by limiting its selection of variables.
