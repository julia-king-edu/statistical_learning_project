---
title: "Final Project"
author: "JÃ¶rdis Strack, Julia King"
format: html
editor: source
---

## 0. Setup Chunk

Text

```{r}
#| output: false
if (!require("pacman")) install.packages("pacman")
pacman::p_load(readr, tidyverse, class, caret, bench, DataExplorer, randomForest, pROC)
```

## 1. Data Exploration

Text

```{r}
#| warning: false
# import dataset
data = read_csv("data/BreastCancer.csv", show_col_types = FALSE)
```

```{r}
# first overview
head(data)
```


```{r}
# explore features and labels
str(data)
```


```{r}
# continue exploration
dim(data)
```

```{r}
# count missing values per column
missing_count_per_column <- colSums(is.na(data))
missing_count_per_column

# INTERPRETATION: There are only missing values for variable
# data$...33 -> caused by trailing comma in csv

data <- subset(data, select = -c(...33))
```

```{r}
#| fig-width: 9
#| fig-height: 7
# visualize distribution of classes

# visualize means
data_means = pivot_longer(data %>% select(3:12), 
                                  everything(), 
                                  names_to = "Variable", 
                                  values_to = "Value")
ggplot(data_means, aes(x = Value)) +
  geom_density(fill = "skyblue", color = "black", alpha = 0.5) +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the variable means")
```

Most means seem to be right-skewed. Thus, they will likely require some form of correction. Taking the logarithm is infeasible because of 0 values. Thus, square roots are shown below:

```{r}
#| fig-width: 9
#| fig-height: 7
ggplot(data_means, aes(x = sqrt(Value))) +
  geom_density(fill = "red", color = "black", alpha = 0.5) +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the square root transformation of the variable means")
```


```{r}
#| fig-width: 9
#| fig-height: 7
# visualize standard errors
data_variances = pivot_longer(data %>% select(13:22), 
                                  everything(), 
                                  names_to = "Variable", 
                                  values_to = "Value")
ggplot(data_variances, aes(x = Value)) +
  geom_density(fill = "skyblue", color = "black") +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the variable variances")
```
Similarly, the standard errors are also right-skewed. They could be fixed with a log-transformation, as standard errors are by definition > 0 (assuming there is variation).

```{r}
#| fig-width: 9
#| fig-height: 7
ggplot(data_variances, aes(x = log(Value))) +
  geom_density(fill = "red", color = "black", alpha = 0.5) +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the log transformations of the variable variances")
```

```{r}
#| fig-width: 9
#| fig-height: 7
# visualize highest values
data_worst = pivot_longer(data %>% select(23:32), 
                                  everything(), 
                                  names_to = "Variable", 
                                  values_to = "Value")
ggplot(data_worst, aes(x = Value)) +
  geom_density(fill = "skyblue", color = "black") +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the highest values of the variables")
```

```{r}
#| fig-width: 9
#| fig-height: 7
ggplot(data_worst, aes(x = sqrt(Value))) +
  geom_density(fill = "red", color = "black", alpha = 0.5) +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the square root transformation of the highest values of the variables")
```

```{r}
#| fig-width: 9
#| fig-height: 7
plot_intro(data)
plot_correlation(data)
```


```{r}
# compute descriptive statistics
summary(data)

# compute share of malignant cancers
malignant_share = sum(data$diagnosis == "M") / nrow(data)
print(paste("The share of malignant cancers in the dataset is ", round(malignant_share, 3), ".", sep = ""))
```

## 2. Data Preprocessing

Text

```{r}
# handle categorical variables appropriately, if present

# change diagnosis to binary variable for easier interpretation
data = data %>% 
  mutate(malignant = as.factor(if_else(diagnosis == "M", TRUE, FALSE))) %>%
  select(-diagnosis) %>%
  relocate(malignant, .before = 2)
```

```{r}
# perform variable selection, if required

# remove id column
data = data %>% select(-id)

# rename variables with whitespaces
colnames(data) = sub(" ", "_", colnames(data))

data = as.data.frame(data, names = TRUE)
```

```{r}
# correlation analysis

# calculate correlation matrix
# correlation_matrix <- cor(data[,2:31])
# correlation_matrix

# find attributes that are highly corrected (ideally >0.75)
# highly_correlated <- findCorrelation(correlation_matrix, cutoff=0.5)
# highly_correlated
```

As seen above in step 1.4) we should most likely reduce the number of variables used for our classifier. This fits the results presented by the heat map of all variabels

```{r}
#| fig-width: 9
#| fig-height: 7
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)

# train the model
model <- train(malignant~., data=data, method="lvq", preProcess="scale", 
                            trControl=control)

# estimate variable importance
importance <- varImp(model, scale=FALSE)

# summarize importance
importance

# plot importance
plot(importance)
```

```{r}
# standardize features

data = data %>%
  mutate_at(vars(-malignant), ~ scale(.))
```


```{r}
# create training and testing sets
set.seed(-12)
train_size = floor(nrow(data) * 0.8)
train_index = sample(1:nrow(data), train_size, replace = FALSE)
test_index = setdiff(1:nrow(data), train_index)

# prepare as dataframes
train_data = data[train_index, ]
test_data = data[test_index, ]

# prepare matrixes and vectors
train_data_x = train_data %>% select(-malignant) %>% as.matrix()
train_data_y = train_data %>% mutate(malignant = as.logical(malignant)) %>% pull(malignant)
test_data_x = test_data %>% select(-malignant) %>% as.matrix()
test_data_y = test_data %>% mutate(malignant = as.logical(malignant)) %>% pull(malignant)

print(paste("The training set contains", nrow(train_data), "entries."))
print(paste("The testing set contains", nrow(test_data), "entries."))
```


## 3. Model Selection & Model Evaluation

### 3.1 K-Nearest Neighbors

Text

# define evaluation method for KNN for given dataset
```{r}
evaluate_KNN <- function(train_x, train_y, test_x, test_y, k_range = 1:10, seed = -12) {
  # check input validity
  if (nrow(train_x) != length(train_y)) {
    stop("Number of elements in classification array train_y does not match number of rows in train_x.")
  }
  if (nrow(test_x) != length(test_y)) {
    stop("Number of elements in classification array test_y does not match number of rows in test_x.")
  }
  if (min(k_range < 1)) {
    stop("Minimum of k_range must be >= 1.")
  }
  if (nrow(train_x) <= max(k_range)) {
    stop("Maximum of k_range is too large for specified nr. of splits and dataset size.")
  }
  # define variables
  mse = c()
  accuracy = c()
  precision = c()
  recall = c()
  n = nrow(test_x)
  
  # run knn & compute prediction matrix
  for (k in k_range) {
    test_y_hat = knn(train = train_x, 
                     test = test_x, 
                     cl = train_y, 
                     k = k)
    test_y_hat = as.logical(test_y_hat)
    pred_matrix = table(Actual = test_y, Predicted = test_y_hat) # columns = predicted, rows = actual, 1 for false, 2 for true. EX: pred_matrix[1, 2] returns false positives
    
    # compute evaluation criteria
    mse = c(mse, mean((test_y - test_y_hat)^2))
    accuracy = c(accuracy, (pred_matrix[1, 1] + pred_matrix[2, 2]) / n)
    precision = c(precision, (pred_matrix[2, 2]) / (pred_matrix[2, 2] + pred_matrix[1, 2]))
    recall = c(recall, (pred_matrix[2, 2]) / (pred_matrix[2, 2] + pred_matrix[2, 1]))
  }
  
  # combine results & return 
  results = data.frame(k = k_range, mse, accuracy, precision, recall)
  return(results)
}
```

```{r}
cross_validate_knn <- function(x, y, n_splits = 10, k_range = 1:10, seed = -12) {
  # validate inputs
  if ((nrow(x) != length(y)) | (length(y) == 0)) {
    stop("Number of elements in classification array train_y does not match number of rows in train_x or is empty.")
  }
  if (n_splits < 1) {
    stop("Number of splits must be larger than 0.")
  }
  if (min(k_range < 1)) {
    stop("Minimum of k_range must be >= 1.")
  }
  if (floor(nrow(x) * (1 - (1 / n_splits))) <= max(k_range)) { # k_range exceeds size of training set in one or more of the splits
    stop("Maximum of k_range is too large for specified nr. of splits and dataset size.")
  }
  
  # initialize variables
  len = nrow(x)
  result = data.frame(k = numeric(), 
                      mse = numeric(), 
                      accuracy = numeric(), 
                      precision = numeric(), 
                      recall = numeric())
  
  # shuffle dataset (index)
  set.seed(seed)
  indexes = sample(1:len)
  
  for (i in 1:n_splits) {
    # split dataset indexes 
    test_index = indexes[floor((i-1)*len/n_splits+1) : floor(i*len/n_splits)]  
    train_index = setdiff(1:len, test_index) 
    # run evaluate_knn
    current_results = evaluate_KNN(train_x = x[train_index, ], train_y = y[train_index], 
                                   test_x = x[test_index, ], test_y = y[test_index], 
                                   k_range = k_range)
    # combine dataframes
    result = rbind(result, current_results)
  }
  # calculate averages and return 
  result = result %>% 
    group_by(k) %>% 
    summarise(across(everything(), mean))
  return(result)
}
```


```{r}
# run cross-validation
knn_eval_cv = cross_validate_knn(train_data_x, train_data_y, 
                                    n_splits = 10, k_range = 1:(nrow(train_data_x) %/% 3))
```



```{r}
#| fig-width: 9
#| fig-height: 7
# visualize results for different K:
ggplot(knn_eval_cv, aes(x = k)) +
  geom_line(aes(y = accuracy, color = "Accuracy")) +
  geom_line(aes(y = precision, color = "Precision")) +
  geom_line(aes(y = recall, color = "Recall")) + 
  # add points where the metrics are maiximized
  geom_point(aes(x = k[which.max(accuracy)], y = max(accuracy), color = "Accuracy"), size = 3) + 
  geom_point(aes(x = k[which.max(precision)], y = max(precision), color = "Precision"), size = 3) + 
  geom_point(aes(x = k[which.max(recall)], y = max(recall), color = "Recall"), size = 3) + 
  # add text to the points indicating the k required to reach said maximum
  geom_text(aes(x = which.max(accuracy), y = max(accuracy), 
                label = paste("(", k[which.max(accuracy)], ",", round(max(accuracy), 3), ")", sep = "")), 
            vjust = 2, hjust = 0) + 
  geom_text(aes(x = which.max(precision), y = max(precision), 
                label = paste("(", k[which.max(precision)], ",", round(max(precision), 3), ")", sep = "")), 
            vjust = 1.5, hjust = 0) + 
  geom_text(aes(x = which.max(recall), y = max(recall), 
                label = paste("(", k[which.max(recall)], ",", round(max(recall), 3), ")", sep = "")), 
            vjust = 1.5, hjust = 0) + 
  # add titles, legend & colors
  labs(title = "Cross-validation results of KNN on training data",
       x = "k",
       y = "score",
       color = "Metric") +
  scale_color_manual(values = c("Accuracy" = "blue", "Precision" = "red", "Recall" = "green"),
                     labels = c("Accuracy", "Precision", "Recall"))
```

```{r}
# keep ideal k for further testing
k = knn_eval_cv$k[which.max(knn_eval_cv$recall)]
```


### 3.2 Random Forest

```{r}
# set up model on training data
RF_model <- randomForest(
  formula = malignant ~ .,
  data = train_data
)

RF_model
```

Looks pretty good! Train data set model accuracy is roughly 96,26%. Go ahead to prediction and confusion matrix

```{r}
pred_RF_train <- predict(RF_model, train_data)
confusionMatrix(pred_RF_train, train_data$malignant)
```

Mini analysis:

```{r}
#| fig-width: 9
#| fig-height: 7
# visualize the tree size 
hist(treesize(RF_model),
     main = "Nodes used for tree construction",
     col = "palegreen")

# again, look at importance of predictors
varImpPlot(RF_model,
           sort = T,
           n.var = 30,
           main = "Importance of predictors")

importance(RF_model)
```

Wow, this looks so different!!

```{r}
# prediction for test data
# pred_RF_test <- predict(RF_model, test_data)
# confusionMatrix(pred_RF_test, test_data$malignant)
```

Setup of cross-validation

```{r}
nfolds = 10

accuracy_fold <- numeric(nfolds)
precision_fold <- numeric(nfolds)
recall_fold <- numeric(nfolds)
f1_fold <- numeric(nfolds)

# Prepare train/test splits for Cross Validation
set.seed(-12)
permutation = sample(1:nrow(train_data))

# perform Cross Validation

for (j in 1:nfolds){
  test_indices <- permutation[((j - 1) * nrow(train_data) / nfolds + 1) : (j * nrow(train_data) / nfolds)]
  train_indices <- setdiff(1:nrow(train_data), test_indices)

  # obtain training and testing folds
  train_fold <- train_data[train_indices, ]
  test_fold <- train_data[test_indices, ]
  
  # fit Random Forest model on training fold
  RF_model <- randomForest(
    formula = malignant ~ .,
    data = train_fold
  )
  
  # predict on pseudo-test folds
  pred <- predict(RF_model, test_fold)
  
  # obtain evaluation metrics for the fold
  cm <- confusionMatrix(data = pred, reference = test_fold$malignant)
  accuracy_fold[j] <- cm$overall["Accuracy"]
  precision_fold[j] <- cm$byClass["Precision"][1]  # precision for class 'M'
  recall_fold[j] <- cm$byClass["Recall"][1]  # recall for class 'M'
  f1_fold[j] <- cm$byClass["F1"][1]  # F1 Score for class 'M'
}

# compute mean of evaluation metrics across folds
mean_accuracy <- mean(accuracy_fold)
mean_precision <- mean(precision_fold)
mean_recall <- mean(recall_fold)
mean_f1 <- mean(f1_fold)

cat("Mean Accuracy:", mean_accuracy, "\n")
cat("Mean Precision:", mean_precision, "\n")
cat("Mean Recall:", mean_recall, "\n")
cat("Mean F1 Score:", mean_f1, "\n")
```

## 4. Fine-Tuning

Text

```{r}
#| fig-width: 9
#| fig-height: 7
tuned_RF_model <- tuneRF(train_data[,-5], train_data[,5],
            stepFactor = 0.5,
            plot = TRUE,
            ntreeTry = 110, # <- I've played around with it & 110 seems to be highest
            trace = TRUE,
            improve = 0.05)
```

## 5. Final Model Selection

Text

```{r}

```

## 5.1 KNN

```{r}
# evaluate on testing data

knn_eval_test = evaluate_KNN(train_x = train_data_x, train_y = train_data_y, 
                                      test_x = test_data_x, test_y = test_data_y, 
                                      k_range = k)
knn_eval_test
```


```{r}
#| fig-width: 9
#| fig-height: 7
# pca for lower dimensionality

get_PVE = function(pca_out) {
  pca_out$sdev ^ 2 / sum(pca_out$sdev ^ 2)
}

train_data_x_pca = prcomp(train_data_x, scale = TRUE)
pve = get_PVE(train_data_x_pca)

knn_pca_percent = 0.9
knn_pca_cutoff = which(cumsum(pve) > knn_pca_percent)[1]

biplot(train_data_x_pca, scale = 0)
```

Hard to read but it's again evident that a lot of variables correlate highly and thus pca might be very efficient at reducing size.

```{r}
ggplot(mapping = aes(x = 1:length(pve), y = cumsum(pve))) + 
  geom_line() + 
  geom_abline(slope = 0, intercept = knn_pca_percent, linetype = "dashed") + 
  geom_point(mapping = aes(x = knn_pca_cutoff, y = cumsum(pve)[knn_pca_cutoff])) + 
  geom_text(mapping = aes(x = knn_pca_cutoff, y = cumsum(pve)[knn_pca_cutoff], 
                          label = paste("(", knn_pca_cutoff, ", ", round(cumsum(pve)[knn_pca_cutoff], 3), ")", sep = "")), 
            vjust = 1.5, hjust = 0)

```

```{r}
# apply evaluations to pca
# continue here

# train_data_x_pca$x[, 1:knn_pca_cutoff]

test_data_x_pca = scale(test_data_x) %*% train_data_x_pca$rotation[, 1:knn_pca_cutoff]

knn_eval_pca = evaluate_KNN(train_x = train_data_x_pca$x[, 1:knn_pca_cutoff], train_y = train_data_y, 
                                      test_x = test_data_x_pca, test_y = test_data_y, 
                                      k_range = k)
knn_eval_pca
```

```{r}
# visualize normal test and pca test to see if dataset reduction costs performance
knn_eval = knn_eval_cv %>% 
  mutate(category = "Cross-Validation") %>% 
  filter(k == .GlobalEnv$k) %>%
  bind_rows(knn_eval_pca %>% mutate(category = "test set (PCA)")) %>%
  bind_rows(knn_eval_test %>% mutate(category = "test set (regular)")) %>%
  select(-k)
```

 precision is 1 for pca?
 
```{r}
#| fig-width: 9
#| fig-height: 5
# visualize
knn_eval %>%
  pivot_longer(cols = -category) %>%
  ggplot(mapping = aes(x = category, y = value, fill = category)) +
  geom_bar(stat = "identity") + 
  ylim(0, NA) + 
  facet_grid(. ~ name) + 
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
```
 


```{r}
#calculate predicted values for knn
knn_predictions = knn(train = train_data_x, 
                      test = test_data_x, 
                      cl = train_data_y, 
                      k = k, 
                      prob = TRUE)

knn_probabilities = attr(knn_predictions, "prob")

knn_compare = data.frame(y_true = test_data_y, y_pred = knn_probabilities) %>%
  arrange(desc(y_pred)) # start with predictions of 1 and end with those of 0
```


```{r}
# roc curve for best k

roc_data <- knn_compare %>%
  mutate(true_positive = cumsum(y_true == 1) / sum(y_true == 1),
         false_positive = cumsum(y_true == 0) / sum(y_true == 0))

ggplot(roc_data, aes(x = false_positive, y = true_positive)) +
  geom_line() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +  # Diagonal line
  labs(x = "False Positive Rate", y = "True Positive Rate", title = "ROC Curve of knn with k = 6")

```

Please let this not be the real curve.