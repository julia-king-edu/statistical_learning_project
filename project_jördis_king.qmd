---
title: "Final Project"
author: "JÃ¶rdis Strack, Julia King"
format: html
editor: source
---

## 0. Setup Chunk

```{r}
#| output: false
if (!require("pacman")) install.packages("pacman")
pacman::p_load(readr, tidyverse, class, caret, bench, DataExplorer, randomForest)
```

## 1. Data Exploration

### 1.1 Loading the dataset

```{r}
#| warning: false
# import dataset
data = read_csv("data/BreastCancer.csv", show_col_types = FALSE)
```

### 1.2 Exploring features and labels

```{r}
# first overview
head(data)
```

```{r}
# explore features and labels
str(data)
```

```{r}
# continue exploration
dim(data)
```

### 1.3 Checking for missing values

```{r}
# count missing values per column
missing_count_per_column <- colSums(is.na(data))
missing_count_per_column

# INTERPRETATION: There are only missing values for variable
# data$...33 -> caused by trailing comma in csv

data <- subset(data, select = -c(...33))
```

### 1.4 Visualizing the distribution of classes and features

```{r}
#| fig-width: 9
#| fig-height: 7
# visualize distribution of classes

# visualize means
data_means = pivot_longer(data %>% select(3:12), 
                                  everything(), 
                                  names_to = "Variable", 
                                  values_to = "Value")
ggplot(data_means, aes(x = Value)) +
  geom_density(fill = "skyblue", color = "black", alpha = 0.5) +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the variable means")
```

Most means seem to be right-skewed. Thus, they will likely require some form of correction. Taking the logarithm is infeasible because of 0 values. Thus, square roots are shown below:

```{r}
#| fig-width: 9
#| fig-height: 7
ggplot(data_means, aes(x = sqrt(Value))) +
  geom_density(fill = "red", color = "black", alpha = 0.5) +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the square root transformation of the variable means")
```

```{r}
#| fig-width: 9
#| fig-height: 7
# visualize standard errors
data_variances = pivot_longer(data %>% select(13:22), 
                                  everything(), 
                                  names_to = "Variable", 
                                  values_to = "Value")
ggplot(data_variances, aes(x = Value)) +
  geom_density(fill = "skyblue", color = "black") +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the variable standard deviations")
```

Similarly, the standard errors are also right-skewed. They could be fixed with a log-transformation, as standard errors are by definition \> 0 (assuming there is variation).

```{r}
#| fig-width: 9
#| fig-height: 7
ggplot(data_variances, aes(x = log(Value))) +
  geom_density(fill = "red", color = "black", alpha = 0.5) +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the log transformations of the variable square roots")
```

```{r}
#| fig-width: 9
#| fig-height: 7
# visualize highest values
data_worst = pivot_longer(data %>% select(23:32), 
                                  everything(), 
                                  names_to = "Variable", 
                                  values_to = "Value")
ggplot(data_worst, aes(x = Value)) +
  geom_density(fill = "skyblue", color = "black") +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the highest values of the variables")
```

```{r}
#| fig-width: 9
#| fig-height: 7
ggplot(data_worst, aes(x = sqrt(Value))) +
  geom_density(fill = "red", color = "black", alpha = 0.5) +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Density Plots of the square root transformation of the highest values of the variables")
```

```{r}
#| fig-width: 9
#| fig-height: 7
plot_intro(data)
plot_correlation(data, title = "Correlation Heatmap of all variables")
```

### 1.5 Computing descriptive statistics

```{r}
# compute descriptive statistics
summary(data)

# compute share of malignant cancers
malignant_share = sum(data$diagnosis == "M") / nrow(data)
print(paste("The share of malignant cancers in the dataset is ", round(malignant_share, 3), ".", sep = ""))
```

## 2. Data Preprocessing

Note: Splitting the dataset is done after all other preprocessing steps

### 2.1 Handling categorical variables

```{r}
# change diagnosis to binary variable for easier interpretation
data = data %>% 
  mutate(malignant = as.factor(if_else(diagnosis == "M", TRUE, FALSE))) %>%
  select(-diagnosis) %>%
  relocate(malignant, .before = 2)
```

### 2.2 Performing variable selection

```{r}
# remove id column
data = data %>% select(-id)

# rename variables with whitespaces
colnames(data) = sub(" ", "_", colnames(data))

data = as.data.frame(data, names = TRUE)
```

```{r}
#| fig-width: 9
#| fig-height: 7
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)

# train the model
model <- train(malignant~., data=data, method="lvq", preProcess="scale", 
                            trControl=control)

# estimate variable importance
importance <- varImp(model, scale=FALSE)

# summarize importance
importance

# plot importance
plot(importance)
```

### 2.3 Standardize features

```{r}
# standardize features

data = data %>%
  mutate_at(vars(-malignant), ~ scale(.))
```

### 2.4 Split into training and testing sets

```{r}
# create training and testing sets
set.seed(-12)
train_size = floor(nrow(data) * 0.8)
train_index = sample(1:nrow(data), train_size, replace = FALSE)
test_index = setdiff(1:nrow(data), train_index)

# prepare as dataframes
train_data = data[train_index, ]
test_data = data[test_index, ]

# prepare matrixes and vectors
train_data_x = train_data %>% select(-malignant) %>% as.matrix()
train_data_y = train_data %>% mutate(malignant = as.logical(malignant)) %>% pull(malignant)
test_data_x = test_data %>% select(-malignant) %>% as.matrix()
test_data_y = test_data %>% mutate(malignant = as.logical(malignant)) %>% pull(malignant)

print(paste("The training set contains", nrow(train_data), "entries."))
print(paste("The testing set contains", nrow(test_data), "entries."))
```

### 2.5 Prepare Principal Component Analysis

```{r}
# prepare pca
pca_obj = prcomp(train_data_x, scale = TRUE)

get_PVE = function(pca_out) {
  pca_out$sdev ^ 2 / sum(pca_out$sdev ^ 2)
}

pca_obj = prcomp(train_data_x, scale = TRUE)

biplot(pca_obj, scale = 0)
```

```{r}
# find cutoff level for chosen percentage of variance explained

pve = get_PVE(pca_obj)

pve_percent = 0.9
pca_cutoff = which(cumsum(pve) > pve_percent)[1]
```

```{r}
# plot cumulative function of percentage of variance explained, including cutoff
ggplot(mapping = aes(x = 1:length(pve), y = cumsum(pve))) + 
  geom_line() + 
  geom_abline(slope = 0, intercept = pve_percent, linetype = "dashed") + 
  geom_point(mapping = aes(x = pca_cutoff, y = cumsum(pve)[pca_cutoff])) + 
  geom_text(mapping = aes(x = pca_cutoff, y = cumsum(pve)[pca_cutoff], 
                          label = paste("(", pca_cutoff, ", ", round(cumsum(pve)[pca_cutoff], 3), ")", sep = "")), 
            vjust = 1.5, hjust = 0) + 
  labs(x = "Nr. of included principal components", y = "Sum of explained variance") + 
  ggtitle("Cumulative proportion of variance explained per number of principal components")
```

```{r}
# select principal components in training set and transform testing set 
test_data_x_pca = scale(test_data_x) %*% pca_obj$rotation[, 1:pca_cutoff]
train_data_x_pca = pca_obj$x[, 1:pca_cutoff]
```

```{r}
# convert train_data_x_pca into data frame for Random Forest
train_data_x_pca_DF <- as.data.frame(train_data_x_pca)
test_data_x_pca_DF <- as.data.frame(test_data_x_pca)

# add y = malignant
train_data_pca_RF <- cbind(train_data_x_pca_DF, malignant = train_data_y)
test_data_pca_RF <- cbind(test_data_x_pca_DF, malignant = test_data_y)

train_data_pca_RF$malignant <- train_data_y
test_data_pca_RF$malignant <- test_data_y
```


## 3. Model Selection & Model Evaluation

### 3.1 K-Nearest Neighbors

#### 3.1.1 Defining Evaluation and Cross-Validation Functions

```{r}
# define evaluation function

evaluate_KNN <- function(train_x, train_y, test_x, test_y, k_range = 1:10, seed = -12) {
  #' for a given training and testing set, computes the mse, accuracy, precision and recall values for all k within the specified range.
  #' Returns a dataframe containing the above estimates for each k.
  
  # check input validity
  if (nrow(train_x) != length(train_y)) {
    stop("Number of elements in classification array train_y does not match number of rows in train_x.")
  }
  if (nrow(test_x) != length(test_y)) {
    stop("Number of elements in classification array test_y does not match number of rows in test_x.")
  }
  if (min(k_range < 1)) {
    stop("Minimum of k_range must be >= 1.")
  }
  if (nrow(train_x) <= max(k_range)) {
    stop("Maximum of k_range is too large for specified nr. of splits and dataset size.")
  }
  # define variables
  mse = c()
  accuracy = c()
  precision = c()
  recall = c()
  n = nrow(test_x)
  
  # run knn & compute prediction matrix
  for (k in k_range) {
    test_y_hat = knn(train = train_x, 
                     test = test_x, 
                     cl = train_y, 
                     k = k)
    test_y_hat = as.logical(test_y_hat)
    pred_matrix = table(Actual = test_y, Predicted = test_y_hat) # columns = predicted, rows = actual, 1 for false, 2 for true. EX: pred_matrix[1, 2] returns false positives
    
    # compute evaluation criteria
    mse = c(mse, mean((test_y - test_y_hat)^2))
    accuracy = c(accuracy, (pred_matrix[1, 1] + pred_matrix[2, 2]) / n)
    precision = c(precision, (pred_matrix[2, 2]) / (pred_matrix[2, 2] + pred_matrix[1, 2]))
    recall = c(recall, (pred_matrix[2, 2]) / (pred_matrix[2, 2] + pred_matrix[2, 1]))
  }
  
  # combine results & return 
  results = data.frame(k = k_range, mse, accuracy, precision, recall)
  return(results)
}
```

```{r}
cross_validate_knn <- function(x, y, n_splits = 10, k_range = 1:10, seed = -12) {
  #' For a given training dataset (split into x and y, with x containing the data and y the correct classifications), conducts cross-validation of the KNN algorithm. Utilizes evaluate_KNN, meaning a k_range can be specified. The number of splits can be specified as well.
  #' Returns a dataframe containing the averages of the evaluation metrics returned by evaluate_KNN.

  # validate inputs
  if ((nrow(x) != length(y)) | (length(y) == 0)) {
    stop("Number of elements in classification array train_y does not match number of rows in train_x or is empty.")
  }
  if (n_splits < 1) {
    stop("Number of splits must be larger than 0.")
  }
  if (min(k_range < 1)) {
    stop("Minimum of k_range must be >= 1.")
  }
  if (floor(nrow(x) * (1 - (1 / n_splits))) <= max(k_range)) { # k_range exceeds size of training set in one or more of the splits
    stop("Maximum of k_range is too large for specified nr. of splits and dataset size.")
  }
  
  # initialize variables
  len = nrow(x)
  result = data.frame(k = numeric(), 
                      mse = numeric(), 
                      accuracy = numeric(), 
                      precision = numeric(), 
                      recall = numeric())
  
  # shuffle dataset (index)
  set.seed(seed)
  indexes = sample(1:len)
  
  for (i in 1:n_splits) {
    # split dataset indexes 
    test_index = indexes[floor((i-1)*len/n_splits+1) : floor(i*len/n_splits)]  
    train_index = setdiff(1:len, test_index) 
    # run evaluate_knn
    current_results = evaluate_KNN(train_x = x[train_index, ], train_y = y[train_index], 
                                   test_x = x[test_index, ], test_y = y[test_index], 
                                   k_range = k_range)
    # combine dataframes
    result = rbind(result, current_results)
  }
  # calculate averages and return 
  result = result %>% 
    group_by(k) %>% 
    summarise(across(everything(), mean))
  return(result)
}
```

#### 3.1.2 Running Cross-Validation on regular dataset

```{r}
# run cross-validation with regular dataset
knn_eval_cv_reg = cross_validate_knn(train_data_x, train_data_y, 
                                    n_splits = 10, k_range = 1:(nrow(train_data_x) %/% 3))
```

```{r}
# define visualization function (done as a function, as it will be reused for pca)
visualize_knn_cv_results <- function(knn_cv_results, title) {
  #' given a dataframe returned by cross_validate_knn, plots the result of said function.
  #' Plots the progression of the evaluation metrics, as well as the highest point for each metric.
  
  # visualize results for different K:
  ggplot(knn_cv_results, aes(x = k)) +
    geom_line(aes(y = accuracy, color = "Accuracy")) +
    geom_line(aes(y = precision, color = "Precision")) +
    geom_line(aes(y = recall, color = "Recall")) + 
    
    # add points where the metrics are maiximized
    geom_point(aes(x = k[which.max(accuracy)], y = max(accuracy), color = "Accuracy"), size = 3) + 
    geom_point(aes(x = k[which.max(precision)], y = max(precision), color = "Precision"), size = 3) +
    geom_point(aes(x = k[which.max(recall)], y = max(recall), color = "Recall"), size = 3) + 
    
    # add text to the points indicating the k required to reach said maximum
    geom_text(aes(x = which.max(accuracy), y = max(accuracy), 
                  label = paste("(", k[which.max(accuracy)], ",", round(max(accuracy), 3), ")", sep = "")), 
              vjust = 2, hjust = 0) + 
    geom_text(aes(x = which.max(precision), y = max(precision), 
                  label = paste("(", k[which.max(precision)], ",", round(max(precision), 3), ")", sep = "")), 
              vjust = 1.5, hjust = 0) + 
    geom_text(aes(x = which.max(recall), y = max(recall), 
                  label = paste("(", k[which.max(recall)], ",", round(max(recall), 3), ")", sep = "")), 
              vjust = 1.5, hjust = 0) + 
    
    # add titles, legend & colors
    labs(title = title,
         x = "k",
         y = "score",
         color = "Metric") +
    scale_color_manual(values = c("Accuracy" = "blue", "Precision" = "red", "Recall" = "green"),
                       labels = c("Accuracy", "Precision", "Recall"))
}
```

```{r}
#| fig-width: 9
#| fig-height: 7

# visualize regular results
visualize_knn_cv_results(knn_eval_cv_reg, "Cross-validation results of KNN on standardized training data")
```

#### 3.1.3 Running Cross-Validation on PCA data

```{r}
# run cross-validation with pca data
knn_eval_cv_pca = cross_validate_knn(train_data_x_pca, train_data_y, 
                                    n_splits = 10, k_range = 1:(nrow(train_data_x) %/% 3))
```

```{r}
#| fig-width: 9
#| fig-height: 7

# visualize
visualize_knn_cv_results(knn_eval_cv_pca, "Cross-validation results of KNN on features obtained by PCA")
```

#### 3.1.4 Selecting best-performing k's & saving evaluation metrics

```{r}
# keep ideal k for further testing
k_reg = knn_eval_cv_reg %>% filter(recall == max(recall)) %>% pull(k)
k_pca = knn_eval_cv_pca %>% filter(recall == max(recall)) %>% pull(k)
```

```{r}
# establish comparison table 
# include best results from regular data
cv_results = knn_eval_cv_reg %>% 
  filter(k == k_reg) %>% 
  mutate(model = "KNN (regular)") %>% 
  select(model, accuracy, precision, recall) %>%
  # merge with best results from pca
  bind_rows( 
    knn_eval_cv_pca %>% 
      filter(k == k_pca) %>% 
      mutate(model = "KNN (PCA)") %>% 
      select(model, accuracy, precision, recall)
  )

cv_results
```

This will later be completed with the CV results from Random Forest.


### 3.2 Random Forest

#### 3.2.1 Fitting Random Forest model on PCA-transformed data

```{r}
# set up model on training data
RF_model <- randomForest(
  formula = malignant ~ .,
  data = train_data
)

RF_model
```

Training data set model accuracy is roughly 96,26%. Go ahead to prediction and confusion matrix

```{r}
pred_RF_train <- predict(RF_model, train_data)
confusionMatrix(pred_RF_train, train_data$malignant)
```

Mini analysis:

```{r}
#| fig-width: 9
#| fig-height: 7
# visualize the tree size 
hist(treesize(RF_model),
     main = "Nodes used for tree construction",
     col = "palegreen")

# again, look at importance of predictors
varImpPlot(RF_model,
           sort = T,
           n.var = 30,
           main = "Importance of predictors")

importance(RF_model)
```

todo: Put nice comment discussing the differences between the trees.

#### 3.2.2 Fitting Random Forest model on PCA-transformed data

```{r}
# Fit Random Forest model on PCA-transformed data
RF_model_PCA <- randomForest(
  formula = malignant ~ .,
  data = train_data_pca_RF
)
RF_model_PCA

# Predict on training data
pred_RF_train_PCA <- predict(RF_model_PCA, train_data_pca_RF, type='response')

preds = ifelse(pred_RF_train_PCA > 0.5, "TRUE", "FALSE")
preds = as.factor(preds)

confusionMatrix(preds, as.factor(train_data_pca_RF$malignant))
```

#### 3.2.2. Fitting Random Forest model on PCA-transformed data

```{r}
# Fit Random Forest model on PCA-transformed data
RF_model_PCA_test <- randomForest(
  formula = malignant ~ .,
  data = test_data_pca_RF
)
RF_model_PCA_test

# Predict on training data
pred_RF_test_PCA <- predict(RF_model_PCA_test, test_data_pca_RF, type='response')

preds_test = ifelse(pred_RF_test_PCA > 0.5, "TRUE", "FALSE")
preds_test = as.factor(preds_test)

PCA_conf_RF <- confusionMatrix(preds_test, as.factor(test_data_pca_RF$malignant))
PCA_conf_RF
```

And of course, for comparison a small model analysis of the Random Forest.

```{r}
#| fig-width: 9
#| fig-height: 7
# visualize the tree size 
hist(treesize(RF_model_PCA),
     main = "Nodes used for tree construction",
     col = "palegreen")

# again, look at importance of predictors
varImpPlot(RF_model_PCA,
           sort = T,
           n.var = 30,
           main = "Importance of predictors")

importance(RF_model_PCA)
```
Finally, obtain performance metrics for PCA test data:

```{r}
accuracy <- PCA_conf_RF$overall['Accuracy']
F1_score <- PCA_conf_RF$byClass['F1']
recall <- PCA_conf_RF$byClass['Sensitivity']

# metrics
cat("Accuracy:", accuracy, "\n")
cat("F1 Score:", F1_score, "\n")
cat("Recall (Sensitivity):", recall, "\n")
```


Setup of cross-validation

```{r}
nfolds = 10

accuracy_fold <- numeric(nfolds)
precision_fold <- numeric(nfolds)
recall_fold <- numeric(nfolds)
f1_fold <- numeric(nfolds)

accuracy_fold_PCA <- numeric(nfolds)
precision_fold_PCA <- numeric(nfolds)
recall_fold_PCA <- numeric(nfolds)
f1_fold_PCA <- numeric(nfolds)

# Prepare train/test splits for Cross Validation
set.seed(-12)
permutation = sample(1:nrow(train_data))

# perform Cross Validation

for (j in 1:nfolds){
  test_indices <- permutation[((j - 1) * nrow(train_data) / nfolds + 1) : (j * nrow(train_data) / nfolds)]
  train_indices <- setdiff(1:nrow(train_data), test_indices)

  # obtain training and testing folds
  train_fold <- train_data[train_indices, ]
  test_fold <- train_data[test_indices, ]
  
  train_fold_PCA <- train_data_pca_RF[train_indices, ]
  test_fold_PCA <- train_data_pca_RF[test_indices, ]
  
  # fit Random Forest model on training fold
  RF_model <- randomForest(
    formula = malignant ~ .,
    data = train_fold
  )
  
  ### NOTE: I am suppressing warnings here, since RF (especially in combination with PCA) makes use of regression and the reduction of features to fewer principle components might affect the model structurally and could cause overfitting. We are aware of this and consider it in the analysis but it makes the output look a bit convoluted!
  
  suppressWarnings({
  # RF_model_PCA
  RF_model_PCA <- randomForest(
  formula = malignant ~ .,
  data = train_fold_PCA
  )})
  
  # predict on pseudo-test folds
  pred <- predict(RF_model, test_fold)
  
  # Predict on training data
  pred_RF_test_PCA <- predict(RF_model_PCA, test_fold_PCA, type='response')
  
  preds_test = ifelse(pred_RF_test_PCA > 0.5, "TRUE",  "FALSE")
  preds_test = as.factor(preds_test)

  # obtain evaluation metrics for the fold
  cm <- confusionMatrix(data = pred, reference = test_fold$malignant)
  accuracy_fold[j] <- cm$overall["Accuracy"]
  precision_fold[j] <- cm$byClass["Precision"][1]  # precision for class 'M'
  recall_fold[j] <- cm$byClass["Recall"][1]  # recall for class 'M'
  f1_fold[j] <- cm$byClass["F1"][1]  # F1 Score for class 'M'

  # obtain confusion matrix for PCA
  cm_PCA <- confusionMatrix(data = preds_test, reference = as.factor(test_fold_PCA$malignant))
  accuracy_fold_PCA[j] <- cm_PCA$overall["Accuracy"]
  precision_fold_PCA[j] <- cm_PCA$byClass["Precision"][1]  # precision for class 'M'
  recall_fold_PCA[j] <- cm_PCA$byClass["Recall"][1]  # recall for class 'M'
  f1_fold_PCA[j] <- cm_PCA$byClass["F1"][1]  # F1 Score for class 'M'
}
```
And compute performance metrics:

```{r}
# compute mean of evaluation metrics across folds
mean_accuracy <- mean(accuracy_fold)
mean_precision <- mean(precision_fold)
mean_recall <- mean(recall_fold)
mean_f1 <- mean(f1_fold)

# compute mean of evaluation metrics across folds
mean_accuracy_pca <- mean(accuracy_fold_PCA)
mean_precision_pca <- mean(precision_fold_PCA)
mean_recall_pca <- mean(recall_fold_PCA)
mean_f1_pca <- mean(f1_fold_PCA)

cat("Mean Accuracy:", mean_accuracy, "\n")
cat("Mean Precision:", mean_precision, "\n")
cat("Mean Recall:", mean_recall, "\n")
cat("Mean F1 Score:", mean_f1, "\n")
```

#### 3.2.3 Saving evaluation metrics

```{r}
# add CV results of Random Forest with regular data
cv_results = cv_results %>%
  bind_rows(
    tibble(model = "Random Forest (regular)", accuracy = mean_accuracy, precision = mean_precision, recall = mean_recall)
  ) %>%
  bind_rows(
    tibble(model = "Random Forest (PCA)", accuracy = mean_accuracy_pca, precision = mean_precision_pca, recall = mean_recall_pca)
  )
```


### 3.3 Comparing the model performances

```{r}
#| fig-width: 9
#| fig-height: 5
# visualize

cv_results %>%
  pivot_longer(cols = -model) %>%
  ggplot(mapping = aes(x = model, y = value, fill = model)) +
  geom_bar(stat = "identity") + 
  facet_grid(. ~ name) + 
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) + 
  labs(y = "Evaluation Metric Value", x = "", fill = "Model / Data Combination") + 
  coord_cartesian(ylim=c(0.9,1)) + 
  ggtitle("Comparison of evaluation metrics of the different methods and datasets")

```

todo: conclusion on why we chose random forest

## 4. Fine-Tuning the Random Forest

We fine-tune the Random Forest using grid-search, this time with more options for RF hyperparameters, which are:

-   ntree: The number of generated trees
-   ntry: The number of variables randomly sampled as candidates per split

```{r}
# Define the parameter grid
param_grid <- expand.grid(
  mtry = c(2, 4, 6),  # Example values for mtry
  ntree = c(100, 200, 300)  # Example values for ntree
  # Add other parameters to tune if needed
)
```

```{r}
# Perform Cross-Validation
set.seed(123)  # for reproducibility
folds <- createFolds(train_data$malignant, k = nfolds)

# Initialize variables to store results
results <- data.frame(accuracy = numeric(nrow(param_grid)),
                      precision = numeric(nrow(param_grid)),
                      recall = numeric(nrow(param_grid)),
                      f1 = numeric(nrow(param_grid)))

```

```{r}
# Grid Search
for (i in seq(nrow(param_grid))) {
  # Extract hyperparameters
  mtry <- param_grid$mtry[i]
  ntree <- param_grid$ntree[i]
  
  # Perform cross-validation
  for (j in 1:nfolds) {
    train_indices <- setdiff(1:nrow(train_data), folds[[j]])
    test_indices <- folds[[j]]
    
    # Fit model
    RF_model <- randomForest(
      formula = malignant ~ .,
      data = train_data[train_indices, ],
      mtry = mtry,
      ntree = ntree,
      improve = TRUE,
      trace = TRUE
    )
    
    # Predict on test fold
    pred <- predict(RF_model, newdata = train_data[test_indices, ])
    
    # Evaluate performance
    cm <- confusionMatrix(data = pred, reference = train_data$malignant[test_indices])
    results[i, "accuracy"] <- results[i, "accuracy"] + cm$overall["Accuracy"]
    results[i, "precision"] <- results[i, "precision"] + cm$byClass["Precision"][1]
    results[i, "recall"] <- results[i, "recall"] + cm$byClass["Recall"][1]
    results[i, "f1"] <- results[i, "f1"] + cm$byClass["F1"][1]
  }
}

```

```{r}
# Compute mean performance metrics across folds
results <- results / nfolds

# Find the best combination of hyperparameters
best_index <- which.max(results$f1)  # or use any other metric of interest

# Extract the best hyperparameters
best_mtry <- param_grid$mtry[best_index]
best_ntree <- param_grid$ntree[best_index]

```

```{r}
# Train the final model with the best hyperparameters
final_RF_model <- randomForest(
  formula = malignant ~ .,
  data = train_data,
  mtry = best_mtry,
  ntree = best_ntree
)
```

## 5. Final Model Selection

\*\*Advantages of Random Forests:\*\* Works well with small sample sizes and issues with high correlations between predictors. Additionally, they are less prone to overfitting.

\*\*Disadvantages of Random Forests:\*\* Loss of interpretability

## 6. Applying to test dataset

Run fine-tuned Random Forest on test data

```{r}
# predict ^ obtain predictions as probabilities
pred_test <- predict(final_RF_model, newdata = test_data)
pred_test_prob <- predict(final_RF_model, newdata = test_data, type = "prob")


# obtain final evaluation metrics
final_cm <- confusionMatrix(data = pred_test, reference = test_data$malignant)
test_f1 <- final_cm$byClass["F1"][1]
test_accuracy <- final_cm$overall["Accuracy"]
test_precision <- final_cm$byClass["Precision"][1]
test_recall <- final_cm$byClass["Sensitivity"][1]
test_specificity <- final_cm$byClass["Specificity"][1]

test_mse  <- 1 - test_accuracy

# print final F1 score
cat("Final F1 Score:", test_f1, "\n")
cat("Final Accuracy:", test_accuracy, "\n")
cat("Final Precision:", test_precision, "\n")
cat("Final Recall:", test_recall, "\n")
cat("Final MSE:", test_mse, "\n")
```

And finally, compute some Analytics and visualize!!

```{r}
#| fig-width: 9
#| fig-height: 7

library(PRROC)

# Predict probabilities on the test set
pred_prob <- predict(final_RF_model, newdata = test_data, type = "prob")

# Extract probabilities for the positive class ('TRUE' class)
pred_prob_true <- pred_prob[, "TRUE"]

# Calculate precision and recall
pr_values <- pr.curve(scores.class0 = pred_prob_true, weights.class0 = ifelse(test_data$malignant == "TRUE", 1, 0), curve=TRUE)

plot(pr_values, col='black')

# Compute ROC curve
roc_values <- roc.curve(scores.class0 = pred_prob_true, weights.class0 = ifelse(test_data$malignant == "TRUE", 1, 0), curve = TRUE)

plot(roc_values, col='black')
```

## 7. Appendix

Let's check if we obtain the same results after selecting only the most important variables!

```{r}
# subset data
data = data %>% select(malignant, perimeter_worst, perimeter_mean, perimeter_se, concave_points_se, smoothness_worst, fractal_dimension_se, symmetry_se, smoothness_se, fractal_dimension_mean, texture_se)

# split into training an test data
set.seed(-12)
train_size = floor(nrow(data) * 0.8)
train_index = sample(1:nrow(data), train_size, replace = FALSE)
test_index = setdiff(1:nrow(data), train_index)

# prepare as dataframes
train_data_subset = data[train_index, ]
test_data_subset = data[test_index, ]

```

Train another RandomForest using the optimal hyperparameters revieled above:

```{r}
# Train the final model with the best hyperparameters
final_RF_model <- randomForest(
  formula = malignant ~ .,
  data = train_data_subset,
  mtry = best_mtry,
  ntree = best_ntree
)

# and obtain predictions

# predict ^ obtain predictions as probabilities
pred_test_subset <- predict(final_RF_model, newdata = test_data_subset)
pred_test_prob_subset <- predict(final_RF_model, newdata = test_data_subset, type = "prob")


# obtain final evaluation metrics
final_cm_subset <- confusionMatrix(data = pred_test_subset, reference = test_data$malignant)
test_f1_subset <- final_cm_subset$byClass["F1"][1]
test_accuracy_subset <- final_cm_subset$overall["Accuracy"]
test_precision_subset <- final_cm_subset$byClass["Precision"][1]
test_recall_subset <- final_cm_subset$byClass["Sensitivity"][1]
test_specificity_subset <- final_cm_subset$byClass["Specificity"][1]

test_mse_subset  <- 1 - test_accuracy

# print final F1 score
cat("Final F1 Score:", test_f1_subset, "\n")
cat("Final Accuracy:", test_accuracy_subset, "\n")
cat("Final Precision:", test_precision_subset, "\n")
cat("Final Recall:", test_recall_subset, "\n")
cat("Final MSE:", test_mse_subset, "\n")

```

Performance decreases - it is thus adviced to let the random forest compute the Gini Impurities and let it sort through the different variables itself than us 'prepruning' it by limiting its selection of variables.
